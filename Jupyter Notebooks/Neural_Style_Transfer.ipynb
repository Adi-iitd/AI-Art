{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Style Transfer",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "u4lFgRpyE4W9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !wget --output-document=imagenet-vgg-verydeep-19.mat 'https://storage.googleapis.com/marketing-files/colab-notebooks/style-transfer/imagenet-vgg-verydeep-19.mat'\n",
        "# # !ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l0RljW1-Gale",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from google.colab import files; uploaded = files.upload()\n",
        "# for fn in uploaded.keys():\n",
        "#   print('User uploaded file \"{name}\" with length {length} bytes'.format(name = fn, length = len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HopN_LdNMa-w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import standard libraries. Set the random seed to 42 for both numpy and tensorflow to\n",
        "# get the deterministic output each time you run this notebook.  \n",
        "import numpy as np; np.random.seed(42); import tensorflow as tf; tf.set_random_seed(42);\n",
        "import matplotlib.pyplot as plt; import pylab; import scipy.misc; from scipy import io;\n",
        "import cv2; import os; from os import listdir;\n",
        "%matplotlib inline\n",
        "\n",
        "# Suppress the irrelevant warnings\n",
        "import warnings; warnings.filterwarnings(\"ignore\");"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fvi2gXF6MeHy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_vgg_model(path):\n",
        "  \n",
        "    \"\"\"\n",
        "    Arguments: \n",
        "    path:  Path of the mat file to load!\n",
        "    \n",
        "    Returns:\n",
        "    graph: The complete graph with pre-trained weights and biases loaded\n",
        "    \"\"\"\n",
        "    \n",
        "#   Load the mat file and store the name of the layers somewhere!\n",
        "    vgg = scipy.io.loadmat(path); vgg_layers = vgg['layers']\n",
        "    \n",
        "#   Get the weights corresponding to the specific layer with the corresponding name \n",
        "    def _weights(layer, expected_layer_name):\n",
        "        \n",
        "        \"\"\"\n",
        "        Arguments: \n",
        "        layer:      Integer, the layer index in the architecture\n",
        "        layer_name: Name of the corresponding layer\n",
        "        \n",
        "        Returns:\n",
        "        weight:     Pre-trained Weight \n",
        "        bias:       Pre-trained bias\n",
        "        \"\"\"\n",
        "        \n",
        "        wb = vgg_layers[0][layer][0][0][2]; \n",
        "        W = wb[0][0]; b = wb[0][1]\n",
        "        layer_name = vgg_layers[0][layer][0][0][0][0]\n",
        "        \n",
        "        assert layer_name == expected_layer_name\n",
        "        return W, b\n",
        "\n",
        "    def _relu(conv2d_layer): \n",
        "        \n",
        "        \"\"\"\n",
        "        Arguments: \n",
        "        conv2d_layer: Input Tensor\n",
        "        \n",
        "        Returns:\n",
        "        output:       Output tensor with the ReLU activation added\n",
        "        \"\"\"\n",
        "        \n",
        "        output = tf.nn.relu(conv2d_layer)\n",
        "        return output\n",
        "\n",
        "    def _conv2d(prev_layer, layer, layer_name):\n",
        "      \n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "        prev_layer: Output activations of the previous layer\n",
        "        layer:      Layer index of the architecture\n",
        "        layer_name: Name of the corresponding layer\n",
        "        \n",
        "        Returns: \n",
        "        output:     Convoluted output\n",
        "        \"\"\"\n",
        "        \n",
        "        W, b = _weights(layer, layer_name); W = tf.constant(W); b = tf.constant(np.reshape(b, (b.size)))\n",
        "        output = tf.nn.conv2d(prev_layer, filter = W, strides = [1, 1, 1, 1], padding = 'SAME') + b\n",
        "        \n",
        "        return output \n",
        "\n",
        "    def _conv2d_relu(prev_layer, layer, layer_name): \n",
        "      \n",
        "        return _relu(_conv2d(prev_layer, layer, layer_name))\n",
        "\n",
        "#   Average_pooling (Average of the elements in the sub-block) to reduce the dimensions of the activations\n",
        "    def _avgpool(prev_layer): \n",
        "        \n",
        "        return tf.nn.avg_pool(prev_layer, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
        "    \n",
        "#   Max_pooling (Maximum of the elements in the sub-block) to reduce the dimensions of the activations\n",
        "    def _maxpool(prev_layer): \n",
        "        \n",
        "        return tf.nn.max_pool(prev_layer, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
        "\n",
        "    graph = {}\n",
        "    graph['input']   = tf.Variable(np.zeros((1, img_height, img_width, img_channels)), dtype = 'float32')\n",
        "    graph['conv1_1']  = _conv2d_relu(graph['input'], 0, 'conv1_1')\n",
        "    graph['conv1_2']  = _conv2d_relu(graph['conv1_1'], 2, 'conv1_2')\n",
        "    graph['avgpool1'] = _avgpool(graph['conv1_2'])\n",
        "    graph['conv2_1']  = _conv2d_relu(graph['avgpool1'], 5, 'conv2_1')\n",
        "    graph['conv2_2']  = _conv2d_relu(graph['conv2_1'], 7, 'conv2_2')\n",
        "    graph['avgpool2'] = _avgpool(graph['conv2_2'])\n",
        "    graph['conv3_1']  = _conv2d_relu(graph['avgpool2'], 10, 'conv3_1')\n",
        "    graph['conv3_2']  = _conv2d_relu(graph['conv3_1'], 12, 'conv3_2')\n",
        "    graph['conv3_3']  = _conv2d_relu(graph['conv3_2'], 14, 'conv3_3')\n",
        "    graph['conv3_4']  = _conv2d_relu(graph['conv3_3'], 16, 'conv3_4')\n",
        "    graph['avgpool3'] = _avgpool(graph['conv3_4'])\n",
        "    graph['conv4_1']  = _conv2d_relu(graph['avgpool3'], 19, 'conv4_1')\n",
        "    graph['conv4_2']  = _conv2d_relu(graph['conv4_1'], 21, 'conv4_2')\n",
        "    graph['conv4_3']  = _conv2d_relu(graph['conv4_2'], 23, 'conv4_3')\n",
        "    graph['conv4_4']  = _conv2d_relu(graph['conv4_3'], 25, 'conv4_4')\n",
        "    graph['avgpool4'] = _avgpool(graph['conv4_4'])\n",
        "    graph['conv5_1']  = _conv2d_relu(graph['avgpool4'], 28, 'conv5_1')\n",
        "    graph['conv5_2']  = _conv2d_relu(graph['conv5_1'], 30, 'conv5_2')\n",
        "    graph['conv5_3']  = _conv2d_relu(graph['conv5_2'], 32, 'conv5_3')\n",
        "    graph['conv5_4']  = _conv2d_relu(graph['conv5_3'], 34, 'conv5_4')\n",
        "    graph['avgpool5'] = _avgpool(graph['conv5_4'])\n",
        "    \n",
        "    return graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OOmRqt-RMijE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img_width = 285; img_height = 250; img_channels = 3; log_path = \"./output\"\n",
        "model = load_vgg_model('./imagenet-vgg-verydeep-19.mat');\n",
        "\n",
        "MEANS = np.array([123.68, 116.779, 103.939]).reshape((1, 1, 1, 3)); \n",
        "img_shape = [1, img_height, img_width, img_channels];\n",
        "\n",
        "def reshape_and_normalize_image(image):\n",
        "  \n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    image: Original image\n",
        "    \n",
        "    Returns:\n",
        "    image: Normalized image (Image minus mean of the image datset that was used to train the model) \n",
        "           after getting reshaped to desired size\n",
        "    \"\"\"\n",
        "    \n",
        "    image = np.reshape(image, ((1,) + image.shape)); image = image - MEANS\n",
        "    \n",
        "    return image\n",
        "\n",
        "\n",
        "content_img = cv2.imread('3.JPG'); content_array = reshape_and_normalize_image(content_img);\n",
        "style_img = cv2.imread('2.JPG'); style_array = reshape_and_normalize_image(style_img);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hl7uTPtqNQeq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_image(path, image):\n",
        "    \n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    path:  path where output image will get saved\n",
        "    image: output image after de-normalizing\n",
        "    \"\"\"\n",
        "    \n",
        "    image = image + MEANS; image = np.clip(image[0], 0, 255).astype('uint8')\n",
        "    cv2.imwrite(path, image)\n",
        "\n",
        "def generate_noise_image(content_arr, noise_ratio):\n",
        "  \n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    content_arr: Original image\n",
        "    noise_ratio: Noise ratio, 1 means output will be complete noise\n",
        "                 0 means output will be content image\n",
        "    \n",
        "    Returns:     Noisier version of the original image\n",
        "    \"\"\"\n",
        "    \n",
        "    noise_img = np.random.uniform(-2., 2., size = img_shape).astype('float32')\n",
        "    noise_img = noise_ratio*noise_img + (1 - noise_ratio)*content_arr\n",
        "    \n",
        "    return noise_img\n",
        "\n",
        "generated_img = generate_noise_image(content_array, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qT16DJT1NTCK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "STYLE_LAYERS = [('conv2_1', 1), ('conv3_1', 1e-1), ('conv4_1', 1e-2)]; CONTENT_LAYERS = [('conv2_1', 1)]\n",
        "\n",
        "sess = tf.Session(); sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4hr_CxVh3b0a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "content_features = {}; sess.run(model['input'].assign(content_array));\n",
        "for layer, coef in CONTENT_LAYERS: content_features[layer] = tf.convert_to_tensor(sess.run(model[layer]))\n",
        "\n",
        "style_features = {}; sess.run(model['input'].assign(style_array))\n",
        "for layer, coef in STYLE_LAYERS: style_features[layer] = tf.convert_to_tensor(sess.run(model[layer]))\n",
        "\n",
        "  \n",
        "# Calculates the content loss\n",
        "def content_loss_per_layer(con_ten, gen_ten):\n",
        "    \n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    con_ten:  Activations corresponding to particular layer of content image\n",
        "    gen_ten:  Activations corresponding to particular layer of generated image\n",
        "    \n",
        "    Returns:\n",
        "    con_loss: Loss computed using the formula mentioned in ReadMe\n",
        "    \"\"\"\n",
        "    \n",
        "    m, n_h, n_w, n_c = con_ten.get_shape().as_list()\n",
        "    con_ten_unrolled = tf.reshape(con_ten, shape = [n_h*n_w, n_c])\n",
        "    gen_ten_unrolled = tf.reshape(gen_ten, shape = [n_h*n_w, n_c])\n",
        "    con_loss = tf.reduce_sum(tf.square(con_ten_unrolled - gen_ten_unrolled))/(4*n_h*n_w*n_c)\n",
        "    \n",
        "    return con_loss\n",
        "  \n",
        "def Content_loss(content_features, CONTENT_LAYERS):\n",
        "  \n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    content_features: \n",
        "    CONTENT_LAYERS:   Layers that have been taken into account while calculating Content loss  \n",
        "    \n",
        "    Returns:\n",
        "    con_loss:     weighted loss of different content loss per layers\n",
        "    \"\"\"\n",
        "    con_loss = 0\n",
        "    for layer, coef in CONTENT_LAYERS: con_loss += coef*content_loss_per_layer(content_features[layer], model[layer])\n",
        "    \n",
        "    return con_loss\n",
        "  \n",
        "\n",
        "def gram_matrix(mat):\n",
        "  \n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    mat:      Original matrix\n",
        "    \n",
        "    Returns:\n",
        "    gram_mat: Gram matrix of the original matrix computed using the formula mentioned in ReadMe\n",
        "    \"\"\"\n",
        "    \n",
        "    m, n_h, n_w, n_c = mat.get_shape().as_list(); mat_reshaped = tf.reshape(tf.transpose(mat), shape = [n_c, n_h*n_w])\n",
        "    gram_mat = tf.matmul(mat_reshaped, mat_reshaped, transpose_a = False, transpose_b = True)\n",
        "    \n",
        "    return gram_mat\n",
        "\n",
        "  \n",
        "def style_loss_per_layer(sty_ten, gen_ten):\n",
        "  \n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    sty_ten:  Activations corresponding to particular layer of style image\n",
        "    gen_ten:  Activations corresponding to particular layer of generated image\n",
        "    \n",
        "    Returns:\n",
        "    sty_loss: Style loss computed using the formula mentioned in ReadMe\n",
        "    \"\"\"\n",
        "    \n",
        "    m, n_h, n_w, n_c = sty_ten.get_shape().as_list(); \n",
        "    sty_ten_gram_mat = gram_matrix(sty_ten); gen_ten_gram_mat = gram_matrix(gen_ten)\n",
        "    sty_loss = tf.reduce_sum(tf.square(sty_ten_gram_mat - gen_ten_gram_mat))/(4*n_c**2*n_h**2*n_w**2)\n",
        "    \n",
        "    return sty_loss\n",
        "\n",
        "def Style_loss(style_features, STYLE_LAYERS):\n",
        "  \n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    style_faetures: \n",
        "    STYLE_LAYERS:   Layers that have been taken into account while calculating style loss  \n",
        "    \n",
        "    Returns:\n",
        "    style_loss:    weighted loss of different style loss per layers\n",
        "    \"\"\"\n",
        "    sty_loss = 0\n",
        "    for layer, coef in STYLE_LAYERS: sty_loss += coef*style_loss_per_layer(style_features[layer], model[layer])\n",
        "    \n",
        "    return sty_loss\n",
        "\n",
        "\n",
        "def totalvar_loss(image): return tf.image.total_variation(image);\n",
        "\n",
        "\n",
        "c_loss = Content_loss(content_features, CONTENT_LAYERS); s_loss = Style_loss(style_features, STYLE_LAYERS);\n",
        "tv_loss = totalvar_loss(tf.convert_to_tensor(generated_img, dtype = tf.float32));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lFY4u-4F3h7j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def total_loss(content_loss, style_loss, tv_loss, alpha, beta, gamma): return alpha*content_loss + beta*style_loss + gamma*tv_loss\n",
        "\n",
        "t_loss = total_loss(c_loss, s_loss, tv_loss, 10, 1, 3.18e-5)\n",
        "train_step = tf.train.AdamOptimizer(2.0).minimize(t_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F-Z3XD0P3lkp",
        "colab_type": "code",
        "outputId": "f63d45c2-7e21-423b-b155-d6a9189877fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "sess.run(tf.global_variables_initializer()); sess.run(model['input'].assign(generated_img))\n",
        "if not os.path.exists(log_path): os.makedirs(log_path);\n",
        "\n",
        "for i in range(1, 51):    \n",
        "  \n",
        "    sess.run(train_step);\n",
        "    \n",
        "    if i % 50 == 0:\n",
        "        Jt, Jc, Js, Jtv = sess.run([t_loss, c_loss, s_loss, tv_loss])\n",
        "        print(\"Iteration \" + str(i) + \" :\"); print(\"total cost = \" + str(Jt))\n",
        "        print(\"content cost = \" + str(Jc)); print(\"style cost = \" + str(Js))\n",
        "        mixed_image = sess.run(model['input']); save_image(\"output/\" + str(i) + \".png\", mixed_image)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 50 :\n",
            "total cost = [385840.34]\n",
            "content cost = 2107.1855\n",
            "style cost = 364622.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2UxDqhYuPbcM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "for i in range(1, 2): files.download('./output/' + str(50*i) + '.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uSX-kDrPPoaB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}