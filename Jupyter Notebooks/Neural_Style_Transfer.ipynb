{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Style Transfer",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "u4lFgRpyE4W9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget --output-document=imagenet-vgg-verydeep-19.mat 'https://storage.googleapis.com/marketing-files/colab-notebooks/style-transfer/imagenet-vgg-verydeep-19.mat'\n",
        "# !ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l0RljW1-Gale",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files; uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(name = fn, length = len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HopN_LdNMa-w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import standard libraries. Set the random seed to 42 for both numpy and tensorflow to\n",
        "# get the deterministic output each time you run this notebook.  \n",
        "import numpy as np; np.random.seed(42); import tensorflow as tf; tf.set_random_seed(42);\n",
        "import matplotlib.pyplot as plt; import pylab; import scipy.misc; from scipy import io;\n",
        "import cv2; import os; from os import listdir;\n",
        "%matplotlib inline\n",
        "\n",
        "# Suppress the irrelevant warnings\n",
        "import warnings; warnings.filterwarnings(\"ignore\");"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fvi2gXF6MeHy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_vgg_model(path):\n",
        "  \n",
        "    \"\"\"\n",
        "    Arguments: \n",
        "    path:  Path of the mat file to load!\n",
        "    \n",
        "    Returns:\n",
        "    graph: The complete graph with pre-trained weights and biases loaded\n",
        "    \"\"\"\n",
        "    \n",
        "#   Load the mat file and store the name of the layers somewhere!\n",
        "    vgg = scipy.io.loadmat(path); vgg_layers = vgg['layers']\n",
        "    \n",
        "#   Get the weights corresponding to the specific layer with the corresponding name \n",
        "    def _weights(layer, expected_layer_name):\n",
        "        \n",
        "        \"\"\"\n",
        "        Arguments: \n",
        "        layer:      Integer, the layer index in the architecture\n",
        "        layer_name: Name of the corresponding layer\n",
        "        \n",
        "        Returns:\n",
        "        weight:     Pre-trained Weight \n",
        "        bias:       Pre-trained bias\n",
        "        \"\"\"\n",
        "        \n",
        "        wb = vgg_layers[0][layer][0][0][2]; \n",
        "        W = wb[0][0]; b = wb[0][1]\n",
        "        layer_name = vgg_layers[0][layer][0][0][0][0]\n",
        "        \n",
        "        assert layer_name == expected_layer_name\n",
        "        return W, b\n",
        "\n",
        "    def _relu(conv2d_layer): \n",
        "        \n",
        "        \"\"\"\n",
        "        Arguments: \n",
        "        conv2d_layer: Input Tensor\n",
        "        \n",
        "        Returns:\n",
        "        output:       Output tensor with the ReLU activation added\n",
        "        \"\"\"\n",
        "        \n",
        "        output = tf.nn.relu(conv2d_layer)\n",
        "        return output\n",
        "\n",
        "    def _conv2d(prev_layer, layer, layer_name):\n",
        "      \n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "        prev_layer: Output activations of the previous layer\n",
        "        layer:      Layer index of the architecture\n",
        "        layer_name: Name of the corresponding layer\n",
        "        \n",
        "        Returns: \n",
        "        output:     Convoluted output\n",
        "        \"\"\"\n",
        "        \n",
        "        W, b = _weights(layer, layer_name); W = tf.constant(W); b = tf.constant(np.reshape(b, (b.size)))\n",
        "        output = tf.nn.conv2d(prev_layer, filter = W, strides = [1, 1, 1, 1], padding = 'SAME') + b\n",
        "        \n",
        "        return output \n",
        "\n",
        "    def _conv2d_relu(prev_layer, layer, layer_name): \n",
        "      \n",
        "        return _relu(_conv2d(prev_layer, layer, layer_name))\n",
        "\n",
        "#   Average_pooling (Average of the elements in the sub-block) to reduce the dimensions of the activations\n",
        "    def _avgpool(prev_layer): \n",
        "        \n",
        "        return tf.nn.avg_pool(prev_layer, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
        "    \n",
        "#   Max_pooling (Maximum of the elements in the sub-block) to reduce the dimensions of the activations\n",
        "    def _maxpool(prev_layer): \n",
        "        \n",
        "        return tf.nn.max_pool(prev_layer, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
        "\n",
        "    graph = {}\n",
        "    graph['input']   = tf.Variable(np.zeros((1, img_height, img_width, img_channels)), dtype = 'float32')\n",
        "    graph['conv1_1']  = _conv2d_relu(graph['input'], 0, 'conv1_1')\n",
        "    graph['conv1_2']  = _conv2d_relu(graph['conv1_1'], 2, 'conv1_2')\n",
        "    graph['avgpool1'] = _avgpool(graph['conv1_2'])\n",
        "    graph['conv2_1']  = _conv2d_relu(graph['avgpool1'], 5, 'conv2_1')\n",
        "    graph['conv2_2']  = _conv2d_relu(graph['conv2_1'], 7, 'conv2_2')\n",
        "    graph['avgpool2'] = _avgpool(graph['conv2_2'])\n",
        "    graph['conv3_1']  = _conv2d_relu(graph['avgpool2'], 10, 'conv3_1')\n",
        "    graph['conv3_2']  = _conv2d_relu(graph['conv3_1'], 12, 'conv3_2')\n",
        "    graph['conv3_3']  = _conv2d_relu(graph['conv3_2'], 14, 'conv3_3')\n",
        "    graph['conv3_4']  = _conv2d_relu(graph['conv3_3'], 16, 'conv3_4')\n",
        "    graph['avgpool3'] = _avgpool(graph['conv3_4'])\n",
        "    graph['conv4_1']  = _conv2d_relu(graph['avgpool3'], 19, 'conv4_1')\n",
        "    graph['conv4_2']  = _conv2d_relu(graph['conv4_1'], 21, 'conv4_2')\n",
        "    graph['conv4_3']  = _conv2d_relu(graph['conv4_2'], 23, 'conv4_3')\n",
        "    graph['conv4_4']  = _conv2d_relu(graph['conv4_3'], 25, 'conv4_4')\n",
        "    graph['avgpool4'] = _avgpool(graph['conv4_4'])\n",
        "    graph['conv5_1']  = _conv2d_relu(graph['avgpool4'], 28, 'conv5_1')\n",
        "    graph['conv5_2']  = _conv2d_relu(graph['conv5_1'], 30, 'conv5_2')\n",
        "    graph['conv5_3']  = _conv2d_relu(graph['conv5_2'], 32, 'conv5_3')\n",
        "    graph['conv5_4']  = _conv2d_relu(graph['conv5_3'], 34, 'conv5_4')\n",
        "    graph['avgpool5'] = _avgpool(graph['conv5_4'])\n",
        "    \n",
        "    return graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OOmRqt-RMijE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img_width = 500; img_height = 350; img_channels = 3; log_path = \"./output\"\n",
        "model = load_vgg_model('./imagenet-vgg-verydeep-19.mat');\n",
        "\n",
        "MEANS = np.array([123.68, 116.779, 103.939]).reshape((1, 1, 1, 3)); \n",
        "img_shape = [1, img_height, img_width, img_channels];\n",
        "\n",
        "def reshape_and_normalize_image(image):\n",
        "  \n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    image: Original image\n",
        "    \n",
        "    Returns:\n",
        "    image: Normalized image (Image minus mean of the image datset that was used to train the model) \n",
        "           after getting reshaped to desired size\n",
        "    \"\"\"\n",
        "    \n",
        "    image = np.reshape(image, ((1,) + image.shape)); image = image - MEANS\n",
        "    \n",
        "    return image\n",
        "\n",
        "\n",
        "content_img = cv2.imread('2 (1).jpg'); content_array = reshape_and_normalize_image(content_img);\n",
        "style_img = cv2.imread('Style.jpg'); style_array = reshape_and_normalize_image(style_img);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hl7uTPtqNQeq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_image(path, image):\n",
        "    \n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    path:  path where output image will get saved\n",
        "    image: output image after de-normalizing\n",
        "    \"\"\"\n",
        "    \n",
        "    image = image + MEANS; image = np.clip(image[0], 0, 255).astype('uint8')\n",
        "    cv2.imwrite(path, image)\n",
        "\n",
        "def generate_noise_image(content_arr, noise_ratio):\n",
        "  \n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    content_arr: Original image\n",
        "    noise_ratio: Noise ratio, 1 means output will be complete noise\n",
        "                 0 means output will be content image\n",
        "    \n",
        "    Returns:     Noisier version of the original image\n",
        "    \"\"\"\n",
        "    \n",
        "    noise_img = np.random.uniform(-20., 20., size = img_shape).astype('float32')\n",
        "    noise_img = noise_ratio*noise_img + (1-noise_ratio)*content_arr\n",
        "    \n",
        "    return noise_img\n",
        "\n",
        "generated_img = generate_noise_image(content_array, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qT16DJT1NTCK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "STYLE_LAYERS = [('conv2_1', 0.25), ('conv3_1', 0.25), ('conv4_1', 0.25), ('conv5_1', 0.25) ]; \n",
        "CONTENT_LAYERS = [('conv3_1', 1)]\n",
        "\n",
        "sess = tf.Session(); sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4hr_CxVh3b0a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "content_features = {}; sess.run(model['input'].assign(content_array));\n",
        "for layer, coef in CONTENT_LAYERS: content_features[layer] = tf.convert_to_tensor(sess.run(model[layer]))\n",
        "  \n",
        "# Calculates the content loss\n",
        "def content_loss(content_tensor, generated_tensor):\n",
        "    \n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    content_tensor:   Activations corresponding to particular layer of content image\n",
        "    generated tensor: Activations corresponding to particular layer of generated image\n",
        "    \n",
        "    Returns:\n",
        "    content_loss:     Loss computed using the formula mentioned in ReadMe\n",
        "    \"\"\"\n",
        "    \n",
        "    m, n_h, n_w, n_c = content_tensor.get_shape().as_list()\n",
        "    content_tensor_unrolled = tf.reshape(content_tensor, shape = [n_h*n_w, n_c])\n",
        "    generated_tensor_unrolled = tf.reshape(generated_tensor, shape = [n_h*n_w, n_c])\n",
        "    content_loss = tf.reduce_sum(tf.square(content_tensor_unrolled - generated_tensor_unrolled))/(4*n_h*n_w*n_c)\n",
        "    \n",
        "    return content_loss\n",
        "\n",
        "def gram_matrix(mat):\n",
        "  \n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    mat:      Original matrix\n",
        "    \n",
        "    Returns:\n",
        "    gram_mat: Gram matrix of the original matrix computed using the formula mentioned in ReadMe\n",
        "    \"\"\"\n",
        "    \n",
        "    m, n_h, n_w, n_c = mat.get_shape().as_list(); mat_reshaped = tf.reshape(tf.transpose(mat), shape = [n_c, n_h*n_w])\n",
        "    gram_mat = tf.matmul(mat_reshaped, mat_reshaped, transpose_a = False, transpose_b = True)\n",
        "    \n",
        "    return gram_mat\n",
        "\n",
        "def style_loss_per_layer(s_t, g_t):\n",
        "  \n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    s_t:    Activations corresponding to particular layer of style image\n",
        "    g_t:    Activations corresponding to particular layer of generated image\n",
        "    \n",
        "    Returns:\n",
        "    s_loss: Style loss computed using the formula mentioned in ReadMe\n",
        "    \"\"\"\n",
        "    \n",
        "    m, n_h, n_w, n_c = s_t.get_shape().as_list(); s_t_gram_mat = gram_matrix(s_t); g_t_gram_mat = gram_matrix(g_t)\n",
        "    s_loss = tf.reduce_sum(tf.square(s_t_gram_mat - g_t_gram_mat))/(4*n_c**2*n_h**2*n_w**2)\n",
        "    \n",
        "    return s_loss\n",
        "\n",
        "def Style_loss(style_features, STYLE_LAYERS):\n",
        "  \n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    style_faetures: \n",
        "    STYLE_LAYERS:   Layers that have been taken into account while calculating style loss  \n",
        "    \n",
        "    Returns:\n",
        "    style_loss:    weighted loss of different style loss per layers\n",
        "    \"\"\"\n",
        "    style_loss = 0\n",
        "    for layer, coef in STYLE_LAYERS: style_loss += coef*style_loss_per_layer(style_features[layer], model[layer])\n",
        "    \n",
        "    return style_loss\n",
        "\n",
        "style_features = {}; sess.run(model['input'].assign(style_array))\n",
        "for layer, coef in STYLE_LAYERS: style_features[layer] = tf.convert_to_tensor(sess.run(model[layer]))\n",
        "\n",
        "def totalvar_loss(image): return tf.image.total_variation(image);\n",
        "\n",
        "c_loss = content_loss(content_features['conv3_1'], model['conv3_1']); \n",
        "s_loss = Style_loss(style_features, STYLE_LAYERS)\n",
        "tv_loss = totalvar_loss(tf.convert_to_tensor(generated_img, preferred_dtype = tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lFY4u-4F3h7j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def total_loss(content_loss, style_loss, tv_loss, alpha, beta, gamma): return alpha*content_loss + beta*style_loss + gamma*tv_loss\n",
        "\n",
        "t_loss = total_loss(c_loss, s_loss, tv_loss, 10, 100, 10)\n",
        "train_step = tf.train.AdamOptimizer(10.0).minimize(t_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F-Z3XD0P3lkp",
        "colab_type": "code",
        "outputId": "94e747b1-5591-40bc-a20c-414291b7b30a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "sess.run(tf.global_variables_initializer()); sess.run(model['input'].assign(generated_img))\n",
        "if not os.path.exists(log_path): os.makedirs(log_path)\n",
        "\n",
        "for i in range(1, 41):    \n",
        "  sess.run(train_step);    \n",
        "  if i % 10 == 0:\n",
        "    Jt, Jc, Js, Jtv = sess.run([t_loss, c_loss, s_loss, tv_loss])\n",
        "    print(\"Iteration \" + str(i) + \" :\"); print(\"total cost = \" + str(Jt))\n",
        "    print(\"content cost = \" + str(Jc)); print(\"style cost = \" + str(Js))\n",
        "    mixed_image = sess.run(model['input']); save_image(\"output/\" + str(i) + \".png\", mixed_image)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 10 :\n",
            "total cost = [8.522509e+09]\n",
            "content cost = 9349.85\n",
            "style cost = 84527384.0\n",
            "Iteration 20 :\n",
            "total cost = [4.475226e+09]\n",
            "content cost = 10605.113\n",
            "style cost = 44054436.0\n",
            "Iteration 30 :\n",
            "total cost = [2.5190085e+09]\n",
            "content cost = 10518.967\n",
            "style cost = 24492270.0\n",
            "Iteration 40 :\n",
            "total cost = [1.6735465e+09]\n",
            "content cost = 10376.962\n",
            "style cost = 16037663.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2UxDqhYuPbcM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "for i in range(1, 5): files.download('./output/' + str(10*i) + '.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uSX-kDrPPoaB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}