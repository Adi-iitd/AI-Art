{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CycleGAN",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "id": "Je38aJHgKxhC",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from google.colab import drive; drive.mount('/content/drive')\n",
        "# import zipfile; zip = zipfile.ZipFile('./drive/My Drive/Dataset.zip'); zip.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kmIS0dYokch7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf; tf.set_random_seed(0); import numpy as np; np.random.seed(0);\n",
        "import matplotlib.pyplot as plt; import pylab; import cv2; import os; from os import listdir;  \n",
        "import warnings; warnings.filterwarnings(\"ignore\");\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DJJaa9odKzSR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_sz = 1; img_height = 128; img_width = 128; img_channels = 3; \n",
        "mon_rel_dir = \"./Dataset/Monet/\"; cez_rel_dir = \"./Dataset/Cezzane/\";\n",
        "\n",
        "mon_file_name = [mon_rel_dir + s for s in os.listdir(mon_rel_dir)]; \n",
        "cez_file_name = [cez_rel_dir + s for s in os.listdir(cez_rel_dir)];"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "so1dLZ_E4Z1y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_tensor_slices(tensor): return tf.data.Dataset.from_tensor_slices(tensor)\n",
        "\n",
        "def get_iterators(sess):\n",
        "    \n",
        "    mon_dataset = get_tensor_slices(tf.constant(mon_file_name)); cez_dataset = get_tensor_slices(tf.constant(cez_file_name));\n",
        "\n",
        "    mon_dataset = mon_dataset.shuffle(500).repeat(); \n",
        "    mon_dataset = mon_dataset.map(lambda x: tf.subtract(tf.div(tf.image.resize_images(tf.image.decode_jpeg(tf.read_file(x)), [img_height, img_width]), 127.5), 1))\n",
        "\n",
        "    cez_dataset = cez_dataset.shuffle(500).repeat();\n",
        "    cez_dataset = cez_dataset.map(lambda x: tf.subtract(tf.div(tf.image.resize_images(tf.image.decode_jpeg(tf.read_file(x)), [img_height, img_width]), 127.5), 1))\n",
        "\n",
        "    mon_dataset = (mon_dataset.batch(batch_sz)).prefetch(1); cez_dataset = (cez_dataset.batch(batch_sz)).prefetch(1);\n",
        "    \n",
        "    handle = tf.placeholder(tf.string, shape = []);\n",
        "    iterator = tf.data.Iterator.from_string_handle(handle, output_types = mon_dataset.output_types, output_shapes = mon_dataset.output_shapes)\n",
        "    next_element = iterator.get_next();\n",
        "    \n",
        "    mon_iterator = mon_dataset.make_initializable_iterator(); mon_handle = sess.run(mon_iterator.string_handle());\n",
        "    cez_iterator = cez_dataset.make_initializable_iterator(); cez_handle = sess.run(cez_iterator.string_handle());\n",
        "    \n",
        "    return handle, next_element, mon_iterator, mon_handle, cez_iterator, cez_handle;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vjU1ENs84VZ_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv_2d(inp_ten, kernel_sz = 4, strides = 1, out_channels = 64, is_conv = True, is_act = True, activation = \"relu\", \n",
        "            leak_param = 1/5.5, is_norm = True, normalization = \"instance\", use_bias = False):\n",
        "    \n",
        "    if is_conv:\n",
        "        x = tf.layers.conv2d(inputs = inp_ten, filters = out_channels, kernel_size = kernel_sz, strides = strides, padding = \"SAME\", \n",
        "                             use_bias = use_bias, kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d());\n",
        "    \n",
        "    if is_norm:\n",
        "        if normalization == \"batch\": x = tf.layers.batch_normalization(x, momentum = 0.9, epsilon = 1e-5, training = train_mode);\n",
        "        elif normalization == \"instance\": x = tf.contrib.layers.instance_norm(x, epsilon = 1e-5);\n",
        "            \n",
        "    if is_act:\n",
        "        if activation == \"relu\": x = tf.nn.relu(x, name = \"relu\");\n",
        "        elif activation == \"leaky_relu\": x = tf.nn.leaky_relu(x, alpha = leak_param, name = \"leaky_relu\");\n",
        "        elif activation == \"elu\": x = tf.nn.elu(x, name = \"elu\");\n",
        "        elif activation == \"tanh\": x = tf.nn.tanh(x, name = \"tanh\");\n",
        "        else: print(\"Check your Activation function\")\n",
        "            \n",
        "    return x\n",
        "  \n",
        "\n",
        "def conv_2d_transpose(inp_ten, kernel_sz = 3, strides = 1, out_channels = 64, is_deconv = True, is_act = True, activation = \"relu\",\n",
        "                      leak_param = 1/5.5, is_norm = True, normalization = \"instance\", is_dropout = False, use_bias = False):\n",
        "    \n",
        "    if is_deconv:\n",
        "        x = tf.layers.conv2d_transpose(inputs = inp_ten, filters = out_channels, kernel_size = kernel_sz, strides = strides, padding = \"SAME\",\n",
        "                                       use_bias = use_bias, kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d());\n",
        "    \n",
        "    if is_norm:\n",
        "        if normalization == \"batch\": x = tf.layers.batch_normalization(x, momentum = 0.9, epsilon = 1e-6, training = train_mode);\n",
        "        elif normalization == \"instance\": x = tf.contrib.layers.instance_norm(x, epsilon = 1e-5);\n",
        "            \n",
        "    if is_act:\n",
        "        if activation == \"relu\": x = tf.nn.relu(x, name = \"relu\");\n",
        "        elif activation == \"leaky_relu\": x = tf.nn.leaky_relu(x, alpha = leak_param, name = \"leaky_relu\");\n",
        "        elif activation == \"elu\": x = tf.nn.elu(x, name = \"elu\");\n",
        "        elif activation == \"tanh\": x = tf.nn.tanh(x, name = \"tanh\");\n",
        "        else: print(\"Check your Activation function\")\n",
        "            \n",
        "    if is_dropout: x = tf.nn.dropout(x, keep_prob = (1 - dropout))\n",
        "            \n",
        "    return x\n",
        "  \n",
        "\n",
        "def res_blk(inp_ten, kernel_sz = 3, strides = 1, out_channels = 128, name = None):\n",
        "    \n",
        "    with tf.variable_scope(name):\n",
        "        \n",
        "        x = conv_2d(inp_ten, kernel_sz = kernel_sz, strides = strides, out_channels = out_channels);\n",
        "        x = conv_2d(x, kernel_sz = kernel_sz, strides = strides, out_channels = out_channels, is_act = False)\n",
        "        \n",
        "        return x + inp_ten;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wqICkEUDtA45",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def Generator(inp_ten, out_channels = 32, name = None, reuse = False):\n",
        "    \n",
        "    with tf.variable_scope(name, reuse = reuse):\n",
        "        \n",
        "        with tf.variable_scope(\"Block_1\"):\n",
        "            x = conv_2d(inp_ten = inp_ten, kernel_sz = 7, strides = 1, out_channels = out_channels*1); \n",
        "            x = conv_2d(inp_ten = x, kernel_sz = 3, strides = 2, out_channels = out_channels*2); \n",
        "            x = conv_2d(inp_ten = x, kernel_sz = 3, strides = 2, out_channels = out_channels*4); \n",
        "\n",
        "        with tf.variable_scope(\"Block_2\"):\n",
        "            for i in range(6): x = res_blk(x, 3, 1, out_channels*4, name = \"ResBlk_\" + str(i));\n",
        "\n",
        "        with tf.variable_scope(\"Block_3\"):\n",
        "            x = conv_2d_transpose(x, kernel_sz = 3, strides = 2, out_channels = 64);\n",
        "            x = conv_2d_transpose(x, kernel_sz = 3, strides = 2, out_channels = 32);\n",
        "            x = conv_2d(x, kernel_sz = 3, strides = 1, out_channels = 3, activation = \"tanh\", is_norm = False);\n",
        "\n",
        "        return x;\n",
        "\n",
        "def Discriminator(inp_ten, out_channels = 64, use_sigmoid = False, name = None, reuse = False):\n",
        "    \n",
        "    with tf.variable_scope(name, reuse = reuse):\n",
        "        \n",
        "        with tf.variable_scope(\"Block_1\"):\n",
        "            x = conv_2d(inp_ten, kernel_sz = 4, strides = 2, out_channels = out_channels, is_norm = False, activation = \"leaky_relu\")\n",
        "\n",
        "        with tf.variable_scope(\"Block_2\"):\n",
        "            for i in range(1, 4): x = conv_2d(x, kernel_sz = 4, strides = 2, out_channels = out_channels*min(2**i, 8),\n",
        "                                              activation = \"leaky_relu\");\n",
        "\n",
        "        with tf.variable_scope(\"Block_3\"):\n",
        "            x = conv_2d(x, kernel_sz = 4, strides = 1, out_channels = 1, is_norm = False, is_act = False, use_bias = True)\n",
        "\n",
        "        if use_sigmoid == True:\n",
        "            x = tf.nn.sigmoid(x); print('Sigmoid activation in the discriminator')\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ps7P737ytG0a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_loss(real_prob, fake_prob):\n",
        "    \n",
        "    with tf.variable_scope(\"Loss\"):\n",
        "      \n",
        "        g_loss =  tf.reduce_mean(tf.squared_difference(fake_prob, 1));\n",
        "        d_loss =  tf.reduce_mean(tf.squared_difference(real_prob, 1)); \n",
        "        d_loss += tf.reduce_mean(tf.square(fake_prob)); d_loss *= 0.5;\n",
        "        return g_loss, d_loss\n",
        "\n",
        "def initialize_model(lambda_1 = 10, lambda_2 = 0.5):\n",
        "    \n",
        "    global input_a, input_b, train_mode, dropout, lr;\n",
        "    \n",
        "    with tf.name_scope(\"Place_holders\"):\n",
        "        input_a = tf.placeholder(dtype = tf.float32, shape = [None, img_height, img_width, img_channels], name = \"Img_A\");\n",
        "        input_b = tf.placeholder(dtype = tf.float32, shape = [None, img_height, img_width, img_channels], name = \"Img_B\");\n",
        "        dropout = tf.placeholder(dtype = tf.float32, name = \"Dropout\"); lr = tf.placeholder(dtype = tf.float32, name = \"Learning_rate\");\n",
        "        train_mode = tf.placeholder(dtype = tf.bool, name = \"Train_Boolean\");\n",
        "        \n",
        "    fake_b  = Generator(inp_ten = input_a, name = \"Generator_a2b\", reuse = False);\n",
        "    recon_a = Generator(inp_ten = fake_b,  name = \"Generator_b2a\", reuse = False);\n",
        "    fake_a  = Generator(inp_ten = input_b, name = \"Generator_b2a\", reuse = True);\n",
        "    recon_b = Generator(inp_ten = fake_a,  name = \"Generator_a2b\", reuse = True);\n",
        "    \n",
        "    fake_b_ = Generator(inp_ten = input_b, name = \"Generator_a2b\", reuse = True);\n",
        "    fake_a_ = Generator(inp_ten = input_a, name = \"Generator_b2a\", reuse = True);\n",
        "\n",
        "    real_prob_a = Discriminator(inp_ten = input_a, name = \"Discriminator_a\", reuse = False);\n",
        "    fake_prob_a = Discriminator(inp_ten = fake_a,  name = \"Discriminator_a\", reuse = True);\n",
        "    real_prob_b = Discriminator(inp_ten = input_b, name = \"Discriminator_b\", reuse = False);\n",
        "    fake_prob_b = Discriminator(inp_ten = fake_b,  name = \"Discriminator_b\", reuse = True);\n",
        "\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(update_ops):\n",
        "\n",
        "        g_b2a_loss, d_a_loss = get_loss(real_prob_a, fake_prob_a); g_a2b_loss, d_b_loss = get_loss(real_prob_b, fake_prob_b);\n",
        "        cycle_consistency_loss = lambda_1*(tf.reduce_mean(tf.abs(input_a - recon_a)) + tf.reduce_mean(tf.abs(input_b - recon_b)));\n",
        "        identity_loss = lambda_2*(tf.reduce_mean(tf.abs(input_a - fake_a_)) + tf.reduce_mean(tf.abs(input_b - fake_b_)));\n",
        "        \n",
        "        g_b2a_loss = g_b2a_loss + cycle_consistency_loss + identity_loss; g_a2b_loss = g_a2b_loss + cycle_consistency_loss + identity_loss;\n",
        "\n",
        "        d_a_vars = [var for var in tf.trainable_variables() if \"Discriminator_a\" in var.name]\n",
        "        d_b_vars = [var for var in tf.trainable_variables() if \"Discriminator_b\" in var.name]\n",
        "        g_b2a_vars = [var for var in tf.trainable_variables() if \"Generator_b2a\" in var.name]\n",
        "        g_a2b_vars = [var for var in tf.trainable_variables() if \"Generator_a2b\" in var.name]\n",
        "\n",
        "        d_a_train_op   = tf.train.AdamOptimizer(learning_rate = lr).minimize(d_a_loss,   var_list = d_a_vars)\n",
        "        d_b_train_op   = tf.train.AdamOptimizer(learning_rate = lr).minimize(d_b_loss,   var_list = d_b_vars)\n",
        "        g_b2a_train_op = tf.train.AdamOptimizer(learning_rate = lr).minimize(g_b2a_loss, var_list = g_b2a_vars)\n",
        "        g_a2b_train_op = tf.train.AdamOptimizer(learning_rate = lr).minimize(g_a2b_loss, var_list = g_a2b_vars)\n",
        "\n",
        "        return fake_b, recon_a, fake_a, recon_b, d_a_loss, d_a_train_op, d_b_loss, d_b_train_op, g_b2a_loss, g_b2a_train_op, g_a2b_loss, g_a2b_train_op;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kcAPaZOswZnV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def show_images(image_batch, **kwargs):\n",
        "\n",
        "    image_batch = (image_batch + 1)*0.5; img_index = 1; fig = plt.figure(figsize = (14, 8), **kwargs); \n",
        "    for _ in range(2):\n",
        "        for _ in range(3):\n",
        "            fig.add_subplot(2, 3, img_index); plt.imshow(image_batch[img_index - 1], cmap = 'binary'); \n",
        "            plt.gca().set_xticks([]); plt.gca().set_yticks([]); img_index += 1;\n",
        "    plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GS5kacuHNBv8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(num_epochs, num_iters):\n",
        "    \n",
        "    g_train = tf.get_default_graph();\n",
        "    with g_train.as_default():\n",
        "        \n",
        "        tf.set_random_seed(0);\n",
        "        with tf.Session(graph = g_train) as sess:\n",
        "            \n",
        "            fake_b, recon_a, fake_a, recon_b, d_a_loss, d_a_train_op, d_b_loss, d_b_train_op, g_b2a_loss, g_b2a_train_op, \\\n",
        "                                                                                g_a2b_loss, g_a2b_train_op = initialize_model();\n",
        "            \n",
        "            handle, next_element, mon_iterator, mon_handle, cez_iterator, cez_handle = get_iterators(sess);\n",
        "            sess.run(tf.global_variables_initializer()); sess.run(mon_iterator.initializer); sess.run(cez_iterator.initializer); \n",
        "            print(\"Training Started...\");\n",
        "            \n",
        "            for iters in range(1, num_epochs*num_iters):\n",
        "                    \n",
        "                if iters/num_iters < 10: curr_lr = 2e-4;\n",
        "                elif iters/num_iters % 5 == 0: curr_lr /= 2;\n",
        "\n",
        "                try: img_a = sess.run(next_element, feed_dict = {handle: mon_handle});\n",
        "                except tf.errors.OutOfRangeError: sess.run(mon_iterator.initializer);\n",
        "\n",
        "                try: img_b = sess.run(next_element, feed_dict = {handle: cez_handle})\n",
        "                except tf.errors.OutOfRangeError: sess.run(cez_iterator.initializer); \n",
        "             \n",
        "                _, D_A_Loss = sess.run([d_a_train_op, d_a_loss],     feed_dict = {input_a: img_a, input_b: img_b, train_mode: True, dropout: 0, lr: curr_lr})\n",
        "                _, G_A_loss = sess.run([g_b2a_train_op, g_b2a_loss], feed_dict = {input_a: img_a, input_b: img_b, train_mode: True, dropout: 0, lr: curr_lr})\n",
        "\n",
        "                _, D_B_Loss = sess.run([d_b_train_op, d_b_loss],     feed_dict = {input_a: img_a, input_b: img_b, train_mode: True, dropout: 0, lr: curr_lr})\n",
        "                _, G_B_Loss = sess.run([g_a2b_train_op, g_a2b_loss], feed_dict = {input_a: img_a, input_b: img_b, train_mode: True, dropout: 0, lr: curr_lr})\n",
        "                \n",
        "                if (4*iters)%num_iters == 0:\n",
        "                  \n",
        "                    Fake_img_B  = sess.run(fake_b, feed_dict = {input_a: img_a, train_mode: True, dropout: 0});\n",
        "                    Fake_img_A  = sess.run(fake_a, feed_dict = {input_b: img_b, train_mode: True, dropout: 0});\n",
        "                    \n",
        "                    Recon_img_B = sess.run(recon_b, feed_dict = {input_b: img_b, train_mode: True, dropout: 0});\n",
        "                    Recon_img_A = sess.run(recon_a, feed_dict = {input_a: img_a, train_mode: True, dropout: 0});\n",
        "\n",
        "                    image_batch = np.concatenate((img_a, Fake_img_B, Recon_img_A, img_b, Fake_img_A, Recon_img_B)); \n",
        "                    show_images(image_batch);\n",
        "                \n",
        "                    print(f'D_A_Loss: {D_A_Loss}, D_B_Loss: {D_B_Loss}, G_B2A_loss: {G_A_loss}, G_A2B_Loss: {G_B_Loss}');\n",
        "                \n",
        "    tf.reset_default_graph(); return;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yG_X4fIENDfQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.get_default_graph(); train(20, min(len(mon_file_name), len(cez_file_name)));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eT6e2UQDcLiL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PsQRw6xUcLeE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}